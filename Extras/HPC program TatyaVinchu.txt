1. Code to implement BFS using OpenMP

#include<iostream>
#include<queue>
using namespace std;

class node {
	
	public:
		node *left, *right;
		int data;
};

class Breadthfs {
	public:
		node *insert(node *, int);
		void bfs(node *);
};
node *insert(node *root, int data) {
	
	if(!root) {
		
		root=new node;
		root->left=NULL;
		root->right=NULL;
		root->data=data;
		return root;
	}

	queue<node *> q; 
	q.push(root);
	
	while(!q.empty()) {
		
		node *temp=q.front();
		q.pop();
		
		if(temp->left==NULL) {
			
			temp->left=new node; 
			temp->left->left=NULL;
			temp->left->right=NULL;
			temp->left->data=data;
			return root;
		}
		else {
			q.push(temp->left);
		}

		if(temp->right==NULL) {
			
			temp->right=new node; 
			temp->right->left=NULL; 
			temp->right->right=NULL; 
			temp->right->data=data; 
			return root;
		}
		else {
			q.push(temp->right);
		}
	}
}

void bfs(node *head) {
	
	queue<node*> q; 
	q.push(head);
	
	int qSize;

	while (!q.empty()) {
		
		qSize = q.size(); 
		
		#pragma omp parallel for
		for (int i = 0; i < qSize; i++) {
			
			node* currNode; 
			
			#pragma omp critical
			{
				currNode = q.front(); 
				q.pop();
				cout<<"\t"<<currNode->data;
			}
			
			#pragma omp critical
			{
				if(currNode->left)
					q.push(currNode->left);
				
				if(currNode->right)
					q.push(currNode->right);
			}
		}
	}
}

int main() {
	
	node *root=NULL; 
	
	int data;
	char ans;

	do {
		
		cout<<"\n enter data=>";
		cin>>data;
		
		root=insert(root,data);
		
		cout<<"do you want insert one more node?";
		cin>>ans;
	} while(ans=='y'||ans=='Y'); 
	
	bfs(root);
	
	return 0;
}

====================================================================================

2. Code to implement DFS using OpenMP

#include <iostream> 
#include <vector> 
#include <stack> 
#include <omp.h>
using namespace std;

const int MAX = 100000; 
vector<int> graph[MAX]; 
bool visited[MAX];

void dfs(int node) {
	
	stack<int> s; 
	s.push(node);

	while (!s.empty()) { 
		
		int curr_node = s.top(); 
		s.pop();

		if (!visited[curr_node]) { 
		
			visited[curr_node] = true;

			if (visited[curr_node]) {
				cout << curr_node << " ";
			}

			#pragma omp parallel for
			for (int i = 0; i < graph[curr_node].size(); i++) { 
				
				int adj_node = graph[curr_node][i];
				
				if (!visited[adj_node]) {
					s.push(adj_node);
				}
			}
		}
	}
}

int main() {
	
	int n, m, start_node;
	cout << "Enter No of Node,Edges,and start node:"; 
	cin >> n >> m >> start_node;

	cout << "Enter Pair of edges:" ; 
	for (int i = 0; i < m; i++) {
	 
		int u, v;
		cin >> u >> v;
		
		graph[u].push_back(v); 
		graph[v].push_back(u);
	}

	#pragma omp parallel for 
	for (int i = 0; i < n; i++) { 
		visited[i] = false;
	}

	dfs(start_node);

	return 0;
}


========================================================================================

3. Code to Implement parallel bubble sort using OpenMP

#include<iostream>
#include<omp.h>
using namespace std;

void bubble(int *, int);
void swap(int &, int &);

void bubble(int *a, int n) {
 		
		for(int i = 0; i < n; i++) {
			
			int first = i % 2;
 			
			#pragma omp parallel for shared(a,first)
 			for(int j = first; j < n-1; j += 2) {
 				
				if(a[j] > a[j+1]) {
					swap(a[j], a[j+1] );
 				}
			}
		}
}

void swap(int &a, int &b) {
	
	int test;
	test=a;
	a=b;
	b=test;
}

int main() {
	
	int *a,n;
	
	cout<<"\n enter total no of elements=>";
	cin>>n;
	
	a=new int[n];
 	cout<<"\n enter elements=>";

	for(int i=0;i<n;i++) {
		cin>>a[i];
	}

	bubble(a,n);

	cout<<"\n sorted array is=>";
	for(int i=0;i<n;i++) {
 		cout<<a[i]<<endl;
	}

	return 0;
}

=======================================================================
4. Code to Implement parallel merge sort using openmp

#include<iostream>
#include<omp.h>
using namespace std;

void mergesort(int a[],int i,int j);
void merge(int a[],int i1,int j1,int i2,int j2);

void mergesort(int a[],int i,int j) {
		
	int mid;
	
	if(i<j) {
		
		mid=(i+j)/2;

		#pragma omp parallel sections
		{
		
			#pragma omp section
			{
				mergesort(a,i,mid);
			}
 		
			#pragma omp section
			{
				mergesort(a,mid+1,j);
			}
 		}
		merge(a,i,mid,mid+1,j);
	}
}

void merge(int a[],int i1,int j1,int i2,int j2) {

	int temp[1000];
	int i,j,k;
	
	i=i1;
	j=i2;
	k=0;

	while(i<=j1 && j<=j2) {

		if(a[i]<a[j]) {
			temp[k++]=a[i++];
		}
		else {
			temp[k++]=a[j++];
		}
	}
	
	while(i<=j1) {
		temp[k++]=a[i++];
	}
 
 	while(j<=j2) {
		temp[k++]=a[j++];
	}
	
	for(i=i1,j=0;i<=j2;i++,j++) {	
		a[i]=temp[j];
	}
}

int main() {
	
	int *a,n,i;
	
	cout<<"\n enter total no of elements=>";
	cin>>n;
	
	a= new int[n];
	cout<<"\n enter elements=>";

	for(i=0;i<n;i++) {
		cin>>a[i];
	}
	
	#pragma omp 
	mergesort(a, 0, n-1);

	cout<<"\n sorted array is=>";
	for(i=0;i<n;i++) {
		cout<<"\n"<<a[i];
	}

	return 0;
}

===================================================================================

5. Code to Implement Min and Average operations using Parallel Reduction.

#include <iostream>
#include <omp.h>
#include <climits>
using namespace std;

void sum_reduction(int arr[], int n) {
	
	int sum = 0;
 	
	#pragma omp parallel for reduction(+: sum)
	for (int i = 0; i < n; i++) {
		sum += arr[i];
	}
	cout << "Sum: " << sum << endl;
}

void average_reduction(int arr[], int n) {
 
	int sum = 0;
	
	#pragma omp parallel for reduction(+: sum)
	for (int i = 0; i < n; i++) {
		sum += arr[i];
	}
	cout << "Average: " << (double)sum / (n) << endl;
}

int main() {
	
	int *arr,n;
	
	cout<<"\n enter total no of elements=>";
	cin>>n;
 	
	arr=new int[n];
	cout<<"\n enter elements=>";
	for(int i=0;i<n;i++) {
		cin>>arr[i];
	}
	
	sum_reduction(arr, n);
	average_reduction(arr, n);
	
	return 0;
}



=======================================================================================

6. Code to Implement Max and Sum operations using Parallel Reduction.

#include <iostream>
#include <omp.h>
#include <climits>
using namespace std;

void sum_reduction(int arr[], int n) {
	
	int sum = 0;
 	
	#pragma omp parallel for reduction(+: sum)
	for (int i = 0; i < n; i++) {
		sum += arr[i];
	}
	cout << "Sum: " << sum << endl;
}

void average_reduction(int arr[], int n) {
 
	int sum = 0;
	
	#pragma omp parallel for reduction(+: sum)
	for (int i = 0; i < n; i++) {
		sum += arr[i];
	}
	cout << "Average: " << (double)sum / (n-1) << endl;
}

int main() {
	
	int *arr,n;
	
	cout<<"\n enter total no of elements=>";
	cin>>n;
 	
	arr=new int[n];
	cout<<"\n enter elements=>";
	for(int i=0;i<n;i++) {
		cin>>arr[i];
	}
	
	sum_reduction(arr, n);
	average_reduction(arr, n);
	
	return 0;
}

=============================================================================

7. Implement HPC application for AI/ML domain.

import tensorflow as tf

model = tf.keras.models.Sequential([tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)), tf.keras.layers.MaxPooling2D((2, 2)), tf.keras.layers.Flatten(), tf.keras.layers.Dense(10, activation='softmax')])

-------------------------------
mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data() 
x_train, x_test = x_train / 255.0, x_test / 255.0 
-------------------------------------
from mpi4py import MPI

comm = MPI.COMM_WORLD
rank = comm.Get_rank()
size = comm.Get_size()
------------------------------------
def train(model, x_train, y_train, rank, size):
    
    n = len(x_train)
    chunk_size = n // size 
    start = rank * chunk_size 
    end = (rank + 1) * chunk_size

    if rank == size - 1:
        end = n

    x_train_chunk = x_train[start:end] 
    y_train_chunk = y_train[start:end]
    
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    model.fit(x_train_chunk, y_train_chunk, epochs=1, batch_size=32)
    train_loss, train_acc = model.evaluate(x_train_chunk, y_train_chunk, verbose=2) 
    train_acc = comm.allreduce(train_acc, op=MPI.SUM)
    
    return train_acc / size
------------------------------------------
epochs = 5

for epoch in range(epochs): 
    
    train_acc = train(model, x_train, y_train, rank, size) 
    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2) 
    test_acc = comm.allreduce(test_acc, op=MPI.SUM)

    if rank == 0:
        print(f"Epoch {epoch + 1}: Train accuracy = {train_acc:.4f}, Test accuracy = {test_acc / size:.4f}")
--------------------------------------------
